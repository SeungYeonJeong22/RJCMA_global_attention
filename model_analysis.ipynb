{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embed_size, dim, num_layers, dropout, residual_embeddings=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.rnn_dim = dim // 2\n",
    "        self.linear = nn.Linear(dim + embed_size, dim)\n",
    "        self.rnn = nn.LSTM(embed_size, self.rnn_dim, num_layers=num_layers, dropout=dropout,\n",
    "                           bidirectional=True, batch_first=True)\n",
    "        self.residual_embeddings = residual_embeddings\n",
    "        self.init_hidden = nn.Parameter(nn.init.xavier_uniform_(torch.zeros(2 * 2 * num_layers, self.rnn_dim)))\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch = inputs.size(0)\n",
    "        h0 = self.init_hidden[:2 * self.num_layers].unsqueeze(1).expand(2 * self.num_layers,\n",
    "                                                                        batch, self.rnn_dim).contiguous()\n",
    "        c0 = self.init_hidden[2 * self.num_layers:].unsqueeze(1).expand(2 * self.num_layers,\n",
    "                                                                        batch, self.rnn_dim).contiguous()\n",
    "\n",
    "        print(\"LSTM inputs : \", inputs.shape)\n",
    "        outputs, hidden_t = self.rnn(inputs, (h0, c0))\n",
    "\n",
    "        if self.residual_embeddings:\n",
    "            outputs = torch.cat([inputs, outputs], dim=-1)\n",
    "        outputs = self.linear(self.dropout(outputs))\n",
    "\n",
    "        return F.normalize(outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "class DenseCoAttn(nn.Module):\n",
    "\tdef __init__(self, dim1, dim2, dropout): #dim1, dim2 = 512, 512\n",
    "\t\tsuper(DenseCoAttn, self).__init__()\n",
    "\t\tdim = dim1 + dim2\n",
    "\t\tself.dropouts = nn.ModuleList([nn.Dropout(p=dropout) for _ in range(2)])\n",
    "\t\tself.query_linear = nn.Linear(dim, dim)\n",
    "\t\tself.key1_linear = nn.Linear(16, 16)\n",
    "\t\tself.key2_linear = nn.Linear(16, 16)\n",
    "\t\tself.value1_linear = nn.Linear(dim1, dim1)\n",
    "\t\tself.value2_linear = nn.Linear(dim2, dim2)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, value1, value2):\n",
    "\t\tprint(\"DenseCoAttn input value1(video) : \", value1.shape) # 16, 16, 512\n",
    "\t\tprint(\"DenseCoAttn input value2(audio) : \", value2.shape)\n",
    "\t\tjoint = torch.cat((value1, value2), dim=-1)\n",
    "\t\t# audio  audio*W*joint\n",
    "\t\tjoint = self.query_linear(joint)\n",
    "\t\tprint(\"DenseCoAttn joint representation : \", joint.shape)\n",
    "\t\tkey1 = self.key1_linear(value1.transpose(1, 2)) # X_v^T\n",
    "\t\tkey2 = self.key2_linear(value2.transpose(1, 2)) # X_a^T \n",
    "\t\tprint(\"DenseCoAttn X_v^T : \", key1.shape) # 16, 512, 16\n",
    "\t\tprint(\"DenseCoAttn X_a^T : \", key2.shape)\n",
    "\n",
    "\t\tvalue1 = self.value1_linear(value1) # 16, 16, 512 (Can't understanding Layer)\n",
    "\t\tvalue2 = self.value2_linear(value2) # (Can't understanding Layer)\n",
    "\t\tprint(\"DenseCoAttn value1 after value_linear : \", value1.shape)\n",
    "\t\tprint(\"DenseCoAttn value2 after value_linear : \", value2.shape)\n",
    "\n",
    "\t\tweighted1, attn1 = self.qkv_attention(joint, key1, value1, dropout=self.dropouts[0])\n",
    "\t\tweighted2, attn2 = self.qkv_attention(joint, key2, value2, dropout=self.dropouts[1])\n",
    "\t\tprint(\"DenseCoAttn weighted1 : \", weighted1.shape)\n",
    "\t\tprint(\"DenseCoAttn weighted2 : \", weighted2.shape)\n",
    "\n",
    "\t\treturn weighted1, weighted2\n",
    "\n",
    "\tdef qkv_attention(self, query, key, value, dropout=None):\n",
    "\t\td_k = query.size(-1)\n",
    "\t\tscores = torch.bmm(key, query) / math.sqrt(d_k)\n",
    "\t\tscores = torch.tanh(scores) # C_v, C_a\n",
    "\t\tif dropout:\n",
    "\t\t\tscores = dropout(scores)\n",
    "\n",
    "\t\tweighted = torch.tanh(torch.bmm(value, scores))\n",
    "\t\treturn self.relu(weighted), scores # self.relu(weighted) == H_v, H_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NormalSubLayer(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout): # dim1, dim2 = 512, 512\n",
    "        super(NormalSubLayer, self).__init__()\n",
    "        self.dense_coattn = DenseCoAttn(dim1, dim2, dropout)\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim1 + dim2, dim1), # 1024, 512\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.Dropout(p=dropout),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim1 + dim2, dim2),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.Dropout(p=dropout),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        weighted1, weighted2 = self.dense_coattn(data1, data2) # weighted1, weighted2 = 1024, 1024\n",
    "        data1 = data1 + self.linears[0](weighted1) # X_att,v^t\n",
    "        data2 = data2 + self.linears[1](weighted2) # X_att,a^t\n",
    "\n",
    "        print(\"DCNLayer X_att,v : \" , data1.shape)\n",
    "        print(\"DCNLayer X_att,a : \" , data2.shape)\n",
    "\n",
    "        return data1, data2\n",
    "\n",
    "\n",
    "class DCNLayer(nn.Module):\n",
    "    def __init__(self, dim1, dim2, num_seq, dropout): # dim1, dim2 = 512, 512\n",
    "        super(DCNLayer, self).__init__()\n",
    "        self.dcn_layers = nn.ModuleList([NormalSubLayer(dim1, dim2, dropout) for _ in range(num_seq)]) # 여기서 t-th iteration만큼 계산\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        for dense_coattn in self.dcn_layers:\n",
    "            data1, data2 = dense_coattn(data1, data2)\n",
    "\n",
    "        return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "class AVGA(nn.Module):\n",
    "\tdef __init__(self, emed_dim, dim):\n",
    "\t\tsuper(AVGA, self).__init__()\n",
    "\t\tself.attn = PositionAttn(emed_dim, dim)\n",
    "\n",
    "\tdef forward(self, video, audio):\n",
    "\t\tfeat = self.attn(video, audio)\n",
    "\n",
    "\t\treturn feat\n",
    "\n",
    "# audio-guided attention\n",
    "class PositionAttn(nn.Module):\n",
    "\tdef __init__(self, embed_dim, dim):\n",
    "\t\tsuper(PositionAttn, self).__init__()\n",
    "\t\tself.affine_audio = nn.Linear(embed_dim, dim)\n",
    "\t\tself.affine_video = nn.Linear(512, dim)\n",
    "\t\tself.affine_v = nn.Linear(dim, 49, bias=False)\n",
    "\t\tself.affine_g = nn.Linear(dim, 49, bias=False)\n",
    "\t\tself.affine_h = nn.Linear(49, 1, bias=False)\n",
    "\t\tself.affine_feat = nn.Linear(512, dim)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, video, audio):\n",
    "\t\tv_t = video.view(video.size(0) * video.size(1), -1, 512).contiguous()\n",
    "\t\tV = v_t\n",
    "\n",
    "\t\t# Audio-guided visual attention\n",
    "\t\tv_t = self.relu(self.affine_video(v_t))\n",
    "\t\ta_t = audio.view(-1, audio.size(-1))\n",
    "\n",
    "\t\ta_t = self.relu(self.affine_audio(a_t))\n",
    "\n",
    "\t\tcontent_v = self.affine_v(v_t) \\\n",
    "\t\t\t\t\t+ self.affine_g(a_t).unsqueeze(2)\n",
    "\n",
    "\t\tz_t = self.affine_h((torch.tanh(content_v))).squeeze(2)\n",
    "\n",
    "\t\talpha_t = F.softmax(z_t, dim=-1).view(z_t.size(0), -1, z_t.size(1))  # attention map\n",
    "\n",
    "\t\tc_t = torch.bmm(alpha_t, V).view(-1, 512)\n",
    "\t\tvideo_t = c_t.view(video.size(0), -1, 512)\n",
    "\n",
    "\t\tvideo_t = self.affine_feat(video_t)\n",
    "\n",
    "\t\treturn video_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a52df20d9533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMultiDomainAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'layer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layer import LSTM\n",
    "\n",
    "class MultiDomainAttention(nn.Module):\n",
    "    def __init__(self, feature_dim, temporal_window):\n",
    "        super(MultiDomainAttention, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.temporal_window = temporal_window\n",
    "        \n",
    "        self.Wq = nn.Linear(feature_dim, feature_dim)\n",
    "        self.Wk = nn.Linear(feature_dim, feature_dim)\n",
    "        self.Wv = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, F_block):\n",
    "        # F_block: [batch_size, temporal_window, feature_dim]\n",
    "        \n",
    "        Q = self.Wq(F.normalize(F_block))   # [batch_size, temporal_window, feature_dim]\n",
    "        K = self.Wk(F.normalize(F_block))\n",
    "        V = self.Wv(F.normalize(F_block))\n",
    "        \n",
    "        beta_t = torch.bmm(Q, K.transpose(1, 2)) / (self.feature_dim ** 0.5)  # [batch_size, temporal_window, temporal_window]\n",
    "\n",
    "        A_t = F.softmax(beta_t, dim=1)  # Temporal attention map\n",
    "        \n",
    "        beta_f = torch.bmm(K.transpose(1, 2), Q) / (self.temporal_window ** 0.5)  # [batch_size, feature_dim, feature_dim]\n",
    "\n",
    "        A_f = F.softmax(beta_f, dim=0)  # Feature attention map\n",
    "        \n",
    "        V_t = torch.bmm(A_t, V)  # Temporal attention applied\n",
    "        V_tf = torch.bmm(V_t, A_f)  # Feature attention applied\n",
    "        \n",
    "        return V_tf\n",
    "\n",
    "\n",
    "class MWTF(nn.Module):\n",
    "    def __init__(self, feature_dim, temporal_window_list):\n",
    "        super(MWTF, self).__init__()\n",
    "        # temporal_window_list: [4, 8, 16]\n",
    "        self.temporal_window_list = temporal_window_list\n",
    "        self.feature_dim = feature_dim\n",
    "        self.extract = LSTM(feature_dim * len(temporal_window_list), 512, 2, 0.1, residual_embeddings=True) # output: (batch, sequence, features)\n",
    "        \n",
    "        self.attention_blocks = nn.ModuleList([\n",
    "            MultiDomainAttention(feature_dim, window_length)\n",
    "            for window_length in temporal_window_list\n",
    "        ])\n",
    "\n",
    "    def forward(self, Feat):\n",
    "        batch_size, T, _ = Feat.size()\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for attention_block, window_length in zip(self.attention_blocks, self.temporal_window_list):\n",
    "            # Split\n",
    "            num_blocks = T // window_length\n",
    "            F_split = Feat.view(batch_size, num_blocks, window_length, -1).contiguous()  # [batch_size, num_blocks, window_length, feature_dim]\n",
    "            \n",
    "            block_results = []\n",
    "            for i in range(num_blocks):\n",
    "                block_result = attention_block(F_split[:, i, :, :])  # [batch_size, window_length, feature_dim]\n",
    "                block_results.append(block_result)\n",
    "            \n",
    "            F_fused = torch.cat(block_results, dim=1)  # [batch_size, total_temporal_steps, feature_dim]\n",
    "            outputs.append(F_fused)\n",
    "            \n",
    "        output = torch.cat(outputs, dim=2)  # [batch_size, T, total_feature_dim]\n",
    "        print(\"MWT output.shape : \", output.shape)\n",
    "        output = self.extract(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "class LSTM_CAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_CAM, self).__init__()\n",
    "        self.coattn = DCNLayer(512, 512, 1, 0.6)\n",
    "        self.avga = AVGA(512, 512)\n",
    "\n",
    "        self.MWT = MWTF(feature_dim=512, temporal_window_lengths=[4,8,16])\n",
    "\n",
    "        self.audio_extract = LSTM(512, 512, 2, 0.1, residual_embeddings=True) # output: (batch, sequence, features)\n",
    "        self.video_extract = LSTM(512, 512, 2, 0.1, residual_embeddings=True) # output: (batch, sequence, features)\n",
    "\n",
    "        self.vregressor = nn.Sequential(nn.Linear(512, 128),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                     nn.Dropout(0.6),\n",
    "                                 nn.Linear(128, 1))\n",
    "\n",
    "        self.Joint = LSTM(1024, 512, 2, dropout=0, residual_embeddings=True)\n",
    "\n",
    "        self.aregressor = nn.Sequential(nn.Linear(512, 128),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                     nn.Dropout(0.6),\n",
    "                                 nn.Linear(128, 1))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(net, init_type='xavier', init_gain=1):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "\n",
    "        def init_func(m):  # define the initialization function\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    init.uniform_(m.weight.data, 0.0, init_gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    init.xavier_uniform_(m.weight.data, gain=init_gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    init.kaiming_uniform_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "                else:\n",
    "                    raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        print('initialize network with %s' % init_type)\n",
    "        net.apply(init_func)  # apply the initialization function <init_func>\n",
    "\n",
    "\n",
    "    def forward(self, f1_norm, f2_norm):\n",
    "        video = F.normalize(f2_norm, dim=-1)\n",
    "        audio = F.normalize(f1_norm, dim=-1)\n",
    "\n",
    "        print(\"audio shape : \", audio.shape)\n",
    "        audio = self.MWT(audio)\n",
    "        video = self.avga(video, audio)\n",
    "        video = self.MWT(video)\n",
    "        \n",
    "        # # Tried with LSTMs also\n",
    "        # audio = self.audio_extract(audio)\n",
    "        # video = self.video_attn(video, audio)\n",
    "        # video = self.video_extract(video)\n",
    "\n",
    "        video, audio = self.coattn(video, audio)\n",
    "\n",
    "        audiovisualfeatures = torch.cat((video, audio), -1)\n",
    "        \n",
    "        audiovisualfeatures = self.Joint(audiovisualfeatures)\n",
    "        vouts = self.vregressor(audiovisualfeatures) #.transpose(0,1))\n",
    "        aouts = self.aregressor(audiovisualfeatures) #.transpose(0,1))\n",
    "\n",
    "        return vouts.squeeze(2), aouts.squeeze(2)  #final_aud_feat.transpose(1,2), final_vis_feat.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tsav import TwoStreamAuralVisualModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'ABAW2020TNT/model2/TSAV_Sub4_544k.pth.tar' # path to the model\n",
    "model = TwoStreamAuralVisualModel(num_channels=4)\n",
    "saved_model = torch.load(model_path)\n",
    "model.load_state_dict(saved_model['state_dict'])\n",
    "\n",
    "new_first_layer = nn.Conv3d(in_channels=3,\n",
    "\t\t\t\t\tout_channels=model.video_model.r2plus1d.stem[0].out_channels,\n",
    "\t\t\t\t\tkernel_size=model.video_model.r2plus1d.stem[0].kernel_size,\n",
    "\t\t\t\t\tstride=model.video_model.r2plus1d.stem[0].stride,\n",
    "\t\t\t\t\tpadding=model.video_model.r2plus1d.stem[0].padding,\n",
    "\t\t\t\t\tbias=False)\n",
    "\n",
    "new_first_layer.weight.data = model.video_model.r2plus1d.stem[0].weight.data[:, 0:3]\n",
    "model.video_model.r2plus1d.stem[0] = new_first_layer\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tmp model shape check\n",
    "# audiodata = torch.empty((16, 16, 1, 64, 104)).cuda()\n",
    "# visualdata = torch.empty((16, 16, 3, 8, 112, 112)).cuda()\n",
    "\n",
    "# with torch.cuda.amp.autocast():\n",
    "#     with torch.no_grad():\n",
    "#         visual_feats = torch.empty((16, 16, 25088), device = visualdata.device)\n",
    "#         aud_feats = torch.empty((16, 16, 512), device = visualdata.device)\n",
    "\n",
    "#         for i in range(16):\n",
    "#             aud_feat, visualfeat, _ = model(audiodata[i,:,:,:], visualdata[i, :, :, :,:,:])\n",
    "#             visual_feats[i,:,:] = visualfeat.view(16, -1)\n",
    "#             aud_feats[i,:,:] = aud_feat\n",
    "\n",
    "# cam = GAT_LSTM_CAM()\n",
    "# result = cam(aud_feats, visual_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils.utils as utils\n",
    "from EvaluationMetrics.cccmetric import ccc\n",
    "\n",
    "import logging\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "learning_rate_decay_start = 5  # 50\n",
    "learning_rate_decay_every = 2 # 5\n",
    "learning_rate_decay_rate = 0.8 # 0.9\n",
    "total_epoch = 30\n",
    "lr = 0.0001\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, scheduler, epoch, lr, cam, time_chk_path):\n",
    "\tprint('\\nEpoch: %d' % epoch)\n",
    "\tglobal Train_acc\n",
    "\t#wandb.watch(audiovisual_model, log_freq=100)\n",
    "\t#wandb.watch(cam, log_freq=100)\n",
    "\n",
    "\t# switch to train mode\n",
    "\t#audiovisual_model.train()\n",
    "\tmodel.eval()\n",
    "\tcam.train()\n",
    "\n",
    "\tepoch_loss = 0\n",
    "\tvout = list()\n",
    "\tvtar = list()\n",
    "\n",
    "\taout = list()\n",
    "\tatar = list()\n",
    "\n",
    "\tif epoch > learning_rate_decay_start and learning_rate_decay_start >= 0:\n",
    "\t\tfrac = (epoch - learning_rate_decay_start) // learning_rate_decay_every\n",
    "\t\tdecay_factor = learning_rate_decay_rate ** frac\n",
    "\t\tcurrent_lr = lr * decay_factor\n",
    "\t\tutils.set_lr(optimizer, current_lr)  # set the decayed rate\n",
    "\telse:\n",
    "\t\tcurrent_lr = lr\n",
    "\t######## chckpoint 없을 때 이거부터 ##########\n",
    "\tutils.set_lr(optimizer, current_lr)\n",
    "\t############################################\n",
    "\tprint('learning_rate: %s' % str(current_lr))\n",
    "\tlogging.info(\"Learning rate\")\n",
    "\tlogging.info(current_lr)\n",
    "\t#torch.cuda.synchronize()\n",
    "\t#t1 = time.time()\n",
    "\tn = 0\n",
    "\tif time_chk_path:\n",
    "\t\ttime_chk_file = os.path.join(time_chk_path, \"time_chk.txt\")\n",
    "\n",
    "\tglobal_vid_fts, global_aud_fts= None, None\n",
    "  \n",
    "\n",
    "\tfor batch_idx, (visualdata, audiodata, labels_V, labels_A) in tqdm(enumerate(train_loader),\n",
    "\t\t\t\t \t\t\t\t\t\t\t\t\t\t total=len(train_loader), position=0, leave=True):\n",
    "     \n",
    "\t\tprint(\"====\" * 20)\n",
    "\t\tprint(\"Batch Index : \", batch_idx)\n",
    "\n",
    "\t\tprint(\"====\" * 20)\n",
    "\t\tprint(\"visualdata : \", visualdata.shape)\n",
    "\t\tprint(\"audiodata : \", audiodata.shape)\n",
    "\t\tprint(\"labels_V : \", labels_V.shape)\n",
    "\t\tprint(\"labels_A : \", labels_A.shape)\n",
    "\n",
    "\t\tprint(\"====\" * 20)\n",
    "\n",
    "\t\toptimizer.zero_grad(set_to_none=True)\n",
    "\t\taudiodata = audiodata.cuda()#.unsqueeze(2)\n",
    "\n",
    "\t\tvisualdata = visualdata.cuda()#permute(0,4,1,2,3).cuda()\n",
    "  \n",
    "\t\tst2 = time.time()\n",
    "\n",
    "\n",
    "\t\twith torch.cuda.amp.autocast():\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tb, seq_t, c, subseq_t, h, w = visualdata.size()\n",
    "\t\t\t\tvisual_feats = torch.empty((b, seq_t, 25088), dtype=visualdata.dtype, device = visualdata.device)\n",
    "\t\t\t\taud_feats = torch.empty((b, seq_t, 512), dtype=visualdata.dtype, device = visualdata.device)\n",
    "\n",
    "\t\t\t\tfor i in range(visualdata.shape[0]):\n",
    "\t\t\t\t\tst1 = time.time()\n",
    "\t\t\t\t\taud_feat, visualfeat, _ = model(audiodata[i,:,:,:], visualdata[i, :, :, :,:,:])\n",
    "\t\t\t\t\ted1 = time.time()\n",
    "\n",
    "\t\t\t\t\tpre_trained_model_time = ed1 - st1\n",
    "\t\t\t\t\tif time_chk_path:\n",
    "\t\t\t\t\t\twith open(time_chk_file, 'a') as f:\n",
    "\t\t\t\t\t\t\tf.write(f\"Time pre_trained_model: {pre_trained_model_time}\\n\")\n",
    "\t\t\t\t\t# visual_feats[i,:,:] = visualfeat\n",
    "\t\t\t\t\tvisual_feats[i,:,:] = visualfeat.view(seq_t, -1)\n",
    "\t\t\t\t\taud_feats[i,:,:] = aud_feat\n",
    "\n",
    "\t\t\tst2 = time.time()\n",
    "\t\t\t# if batch_idx==0:\n",
    "\t\t\t\t# audiovisual_vouts,audiovisual_aouts, global_vid_fts, global_aud_fts = cam(aud_feats, visual_feats)\n",
    "\t\t\t# else:\n",
    "\t\t\t\t# audiovisual_vouts,audiovisual_aouts, global_vid_fts, global_aud_fts = cam(aud_feats, visual_feats, global_vid_fts, global_aud_fts)\n",
    "\t\n",
    "\t\t\taudiovisual_vouts,audiovisual_aouts = cam(aud_feats, visual_feats)\n",
    "\t\t\tprint(\"audiovisual_vouts : \" , audiovisual_vouts.shape)\n",
    "\t\t\tprint(\"audiovisual_aouts : \" , audiovisual_aouts.shape)\n",
    "\t\t\ted2 = time.time()\n",
    "   \n",
    "\t\t\ttime_cam_model= ed2 - st2\n",
    "\t\t\tif time_chk_path:\n",
    "\t\t\t\twith open(time_chk_file, 'a') as f:\n",
    "\t\t\t\t\tf.write(f\"Time cam model: {time_cam_model}\\n\")\n",
    "\t\t\t\t\tf.write(f\"Epoch: {epoch}\\n\")\n",
    "\t\t\t\t\tf.write(f\"batch_idx: {batch_idx}\\n\")\n",
    "\t\t\t\t\tf.write(\"----\"*20)\n",
    "\t\t\t\t\tf.write(\"\\n\")\n",
    "\t\t\t\tf.close()\n",
    "\n",
    "\t\t\tvoutputs = audiovisual_vouts.view(-1, audiovisual_vouts.shape[0]*audiovisual_vouts.shape[1])\n",
    "\t\t\taoutputs = audiovisual_aouts.view(-1, audiovisual_aouts.shape[0]*audiovisual_aouts.shape[1])\n",
    "\t\t\tvtargets = labels_V.view(-1, labels_V.shape[0]*labels_V.shape[1]).cuda()\n",
    "\t\t\tatargets = labels_A.view(-1, labels_A.shape[0]*labels_A.shape[1]).cuda()\n",
    "   \n",
    "\t\t\tv_loss = criterion(voutputs, vtargets)\n",
    "\t\t\ta_loss = criterion(aoutputs, atargets)\n",
    "   \n",
    "\t\t\tfinal_loss = v_loss + a_loss\n",
    "   \n",
    "\t\t\tepoch_loss += final_loss.cpu().data.numpy()\n",
    "\n",
    "\t\t# scaler.scale(final_loss).backward(retain_graph=True)\n",
    "\t\t# scaler.step(optimizer)\n",
    "\t\t# scaler.update()\n",
    "\n",
    "\t\twith torch.autograd.set_detect_anomaly(True):\n",
    "\t\t\tfinal_loss.backward(retain_graph=True)\n",
    "\t\t\toptimizer.step()\n",
    "\t\tn = n + 1\n",
    "\n",
    "\t\tvout = vout + voutputs.squeeze(0).detach().cpu().tolist()\n",
    "\t\tvtar = vtar + vtargets.squeeze(0).detach().cpu().tolist()\n",
    "\n",
    "\t\taout = aout + aoutputs.squeeze(0).detach().cpu().tolist()\n",
    "\t\tatar = atar + atargets.squeeze(0).detach().cpu().tolist()\n",
    "\n",
    "\t\tbreak\n",
    "  \n",
    "\tscheduler.step(epoch_loss / n)\n",
    "\n",
    "\tif (len(vtar) > 1):\n",
    "\t\ttrain_vacc = ccc(vout, vtar)\n",
    "\t\ttrain_aacc = ccc(aout, atar)\n",
    "\telse:\n",
    "\t\ttrain_acc = 0\n",
    "\tprint(\"Train Accuracy\")\n",
    "\tprint(train_vacc)\n",
    "\tprint(train_aacc)\n",
    " \n",
    "\treturn train_vacc, train_aacc, final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with xavier\n",
      "Fusion Model :  LSTM_CAM\n",
      "==> Preparing data..\n",
      "Train Data\n",
      "Number of Sequences: 247\n",
      "Number of Train samples:68709\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "from models.tsav import TwoStreamAuralVisualModel\n",
    "from datasets.dataset_new import ImageList\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from losses.loss import CCCLoss\n",
    "from datetime import datetime, timedelta\n",
    "from torch import nn\n",
    "import json\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "is_time_chk = False\n",
    "\n",
    "best_Val_acc = 0  # best PrivateTest accuracy\n",
    "#best_Val_acc = 0  # best PrivateTest accuracy\n",
    "best_Val_acc_epoch = 0\n",
    "\n",
    "TrainingAccuracy_V = []\n",
    "TrainingAccuracy_A = []\n",
    "ValidationAccuracy_V = []\n",
    "ValidationAccuracy_A = []\n",
    "\n",
    "Logfile_name = \"LogFiles/\" + \"log_file.log\"\n",
    "logging.basicConfig(filename=Logfile_name, level=logging.INFO)\n",
    "\n",
    "SEED = int(0)\n",
    "    \n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "class TrainPadSequence:\n",
    "\tdef __call__(self, sorted_batch):\n",
    "\t\tsequences = [x[0] for x in sorted_batch]\n",
    "\t\taud_sequences = [x[1] for x in sorted_batch]\n",
    "\t\tspec_dim = []\n",
    "\n",
    "\t\tfor aud in aud_sequences:\n",
    "\t\t\tspec_dim.append(aud.shape[3])\n",
    "\n",
    "\t\tmax_spec_dim = max(spec_dim)\n",
    "\t\taudio_features = torch.zeros(len(spec_dim), 16, 1, 64, max_spec_dim)\n",
    "\t\tfor batch_idx, spectrogram in enumerate(aud_sequences):\n",
    "\t\t\tif spectrogram.shape[2] < max_spec_dim:\n",
    "\t\t\t\taudio_features[batch_idx, :, :, :, -spectrogram.shape[3]:] = spectrogram\n",
    "\t\t\telse:\n",
    "\t\t\t\taudio_features[batch_idx, :,:, :, :] = spectrogram\n",
    "\n",
    "\t\tlabelV = [x[2] for x in sorted_batch]\n",
    "\t\tlabelA = [x[3] for x in sorted_batch]\n",
    "\t\tvisual_sequences = torch.stack(sequences)\n",
    "\t\tlabelsV = torch.stack(labelV)\n",
    "\t\tlabelsA = torch.stack(labelA)\n",
    "\n",
    "\t\treturn visual_sequences, audio_features, labelsV, labelsA\n",
    "\n",
    "\n",
    "class ValPadSequence:\n",
    "\tdef __call__(self, sorted_batch):\n",
    "\n",
    "\t\tsequences = [x[0] for x in sorted_batch]\n",
    "\t\taud_sequences = [x[1] for x in sorted_batch]\n",
    "\t\tspec_dim = []\n",
    "\t\tfor aud in aud_sequences:\n",
    "\t\t\tspec_dim.append(aud.shape[3])\n",
    "\n",
    "\t\tmax_spec_dim = max(spec_dim)\n",
    "\t\taudio_features = torch.zeros(len(spec_dim), 16, 1, 64, max_spec_dim)\n",
    "\t\tfor batch_idx, spectrogram in enumerate(aud_sequences):\n",
    "\t\t\tif spectrogram.shape[2] < max_spec_dim:\n",
    "\t\t\t\taudio_features[batch_idx, :, :, :, -spectrogram.shape[3]:] = spectrogram\n",
    "\t\t\telse:\n",
    "\t\t\t\taudio_features[batch_idx, :,:, :, :] = spectrogram\n",
    "\n",
    "\t\tframeids = [x[2] for x in sorted_batch]\n",
    "\t\tv_ids = [x[3] for x in sorted_batch]\n",
    "\t\tv_lengths = [x[4] for x in sorted_batch]\n",
    "\t\tlabelV = [x[5] for x in sorted_batch]\n",
    "\t\tlabelA = [x[6] for x in sorted_batch]\n",
    "\n",
    "\t\tvisual_sequences = torch.stack(sequences)\n",
    "\t\tlabelsV = torch.stack(labelV)\n",
    "\t\tlabelsA = torch.stack(labelA)\n",
    "\t\treturn visual_sequences, audio_features, frameids, v_ids, v_lengths, labelsV, labelsA\n",
    "\n",
    "\n",
    "if not os.path.isdir(\"SavedWeights\"):\n",
    "\tos.makedirs(\"SavedWeights\", exist_ok=True)\n",
    "\n",
    "weight_save_path = \"SavedWeights\"\n",
    "\n",
    "result_save_path =\"save\"\n",
    "if not os.path.exists(result_save_path):\n",
    "    os.makedirs(result_save_path)\n",
    "\n",
    "### Loading audiovisual model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = 'ABAW2020TNT/model2/TSAV_Sub4_544k.pth.tar' # path to the model\n",
    "model = TwoStreamAuralVisualModel(num_channels=4)\n",
    "saved_model = torch.load(model_path)\n",
    "model.load_state_dict(saved_model['state_dict'])\n",
    "\n",
    "new_first_layer = nn.Conv3d(in_channels=3,\n",
    "\t\t\t\t\tout_channels=model.video_model.r2plus1d.stem[0].out_channels,\n",
    "\t\t\t\t\tkernel_size=model.video_model.r2plus1d.stem[0].kernel_size,\n",
    "\t\t\t\t\tstride=model.video_model.r2plus1d.stem[0].stride,\n",
    "\t\t\t\t\tpadding=model.video_model.r2plus1d.stem[0].padding,\n",
    "\t\t\t\t\tbias=False)\n",
    "\n",
    "new_first_layer.weight.data = model.video_model.r2plus1d.stem[0].weight.data[:, 0:3]\n",
    "model.video_model.r2plus1d.stem[0] = new_first_layer\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "### Freezing the model\n",
    "for p in model.parameters():\n",
    "\tp.requires_grad = False\n",
    "for p in model.children():\n",
    "\tp.train(False)\n",
    " \n",
    "fusion_model = LSTM_CAM()\n",
    "\n",
    "print_model_name = fusion_model.__class__.__name__\n",
    "print(\"Fusion Model : \", print_model_name)\n",
    "\n",
    "fusion_model = fusion_model.to(device=device)\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "def matching_files(root_path, anno_path):\n",
    "\tanno_list = []\n",
    "\tfor f in os.listdir(anno_path):\n",
    "\t\tanno_list.append(f.split(\".\")[0])\n",
    "  \n",
    "\troot_path_list = os.listdir(root_path)\n",
    "\t\n",
    "\tfor f in os.listdir(root_path):\n",
    "\t\tif not f in anno_list:\n",
    "\t\t\tdel root_path_list[root_path_list.index(f)]\n",
    "\n",
    "\treturn root_path_list\n",
    "\n",
    "def train_val_test_split(root_path, anno_path, seed=0):\n",
    "\trandom.seed(seed)\n",
    "\ttrial_data = matching_files(root_path, anno_path)\n",
    " \n",
    "\tfname_dict = {i:f for i,f in enumerate(trial_data)}\n",
    "\tlength = len(fname_dict)\n",
    " \n",
    "\tprint(\"full trial length: \", len(fname_dict))\n",
    "\n",
    "\ttrain_set = []\n",
    "\tvalid_set = []\n",
    "\ttest_set = []\n",
    " \n",
    "\ttrain_list_idx = random.sample(fname_dict.keys(), int(length*0.6))\n",
    "\tfor i in train_list_idx:\n",
    "\t\ttrain_set.append(fname_dict[i]+\".csv\")\n",
    "\t\tdel fname_dict[i]\n",
    "\t\t\n",
    "\tvalid_list_idx = random.sample(fname_dict.keys(), int(length*0.2))\n",
    "\tfor i in valid_list_idx:\n",
    "\t\tvalid_set.append(fname_dict[i]+\".csv\")\n",
    "\t\tdel fname_dict[i]\n",
    "\n",
    "\ttest_list_idx = random.sample(fname_dict.keys(), int(length*0.2))    \n",
    "\tfor i in test_list_idx:\n",
    "\t\ttest_set.append(fname_dict[i]+\".csv\")\n",
    "\t\tdel fname_dict[i]\n",
    "  \n",
    "\treturn train_set, valid_set, test_set\n",
    "    \n",
    "with open('config_file.json', 'r') as f:\n",
    "\tconfiguration = json.load(f)\n",
    "\n",
    "dataset_rootpath = configuration['dataset_rootpath']\n",
    "dataset_wavspath = configuration['dataset_wavspath']\n",
    "dataset_labelpath = configuration['labelpath']\n",
    "\n",
    "def load_partition_set(partition_path, seed):\n",
    "\timport json\n",
    "\n",
    "\twith open(partition_path, 'r') as f:    \n",
    "\t\tseed_data = json.load(f)\n",
    "\n",
    "\tseed_data_train = seed_data[f'seed_{seed}']['Train_Set']\n",
    "\tseed_data_valid = seed_data[f'seed_{seed}']['Validation_Set']\n",
    "\tseed_data_test  = seed_data[f'seed_{seed}']['Test_Set']\n",
    " \n",
    "\tseed_data_train = [fn + \".csv\" for fn in seed_data_train]\n",
    "\tseed_data_valid = [fn + \".csv\" for fn in seed_data_valid]\n",
    "\tseed_data_test  = [fn + \".csv\" for fn in seed_data_test ]\n",
    "\n",
    "\treturn seed_data_train, seed_data_valid, seed_data_test\n",
    "\n",
    "partition_path = \"../data/Affwild2/seed_data.json\"\n",
    " \n",
    "train_set, valid_set, test_set = load_partition_set(partition_path, SEED)\n",
    "\n",
    "init_time = datetime.now()\n",
    "init_time = init_time.strftime('%m%d_%H%M')\n",
    "\n",
    "root_time_chk_dir = \"time_chk\"\n",
    "\n",
    "time_chk_path = None\n",
    "\n",
    "print(\"Train Data\")\n",
    "traindataset = ImageList(root=configuration['dataset_rootpath'], fileList=train_set, labelPath=dataset_labelpath,\n",
    "                        audList=configuration['dataset_wavspath'], length=configuration['train_params']['seq_length'],\n",
    "                        flag='train', stride=configuration['train_params']['stride'], dilation = configuration['train_params']['dilation'],\n",
    "                        subseq_length = configuration['train_params']['subseq_length'], time_chk_path=time_chk_path)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                traindataset, collate_fn=TrainPadSequence(),\n",
    "                **configuration['train_params']['loader_params'])\n",
    "print(\"Number of Train samples:\" + str(len(traindataset)))\n",
    "\n",
    "criterion = CCCLoss(digitize_num=1).cuda()\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(),# filter(lambda p: p.requires_grad, multimedia_model.parameters()),\n",
    "\t\t\t\t\t\t\t\tconfiguration['model_params']['lr'])\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "cnt = 0\n",
    "fusion_model_name = 'gat'\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "learning_rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4295 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Batch Index :  0\n",
      "================================================================================\n",
      "visualdata :  torch.Size([16, 16, 3, 8, 112, 112])\n",
      "audiodata :  torch.Size([16, 16, 1, 64, 104])\n",
      "labels_V :  torch.Size([16, 16])\n",
      "labels_A :  torch.Size([16, 16])\n",
      "================================================================================\n",
      "audio shape :  torch.Size([16, 16, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4295 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-783920601583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mTraining_vacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTraining_aacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfusion_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_chk_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_chk_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-98a232ab413d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, scheduler, epoch, lr, cam, time_chk_path)\u001b[0m\n\u001b[1;32m    109\u001b[0m                                 \u001b[0;31m# audiovisual_vouts,audiovisual_aouts, global_vid_fts, global_aud_fts = cam(aud_feats, visual_feats, global_vid_fts, global_aud_fts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                         \u001b[0maudiovisual_vouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maudiovisual_aouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maud_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"audiovisual_vouts : \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maudiovisual_vouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"audiovisual_aouts : \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maudiovisual_aouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Multimodal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a0f94b6863c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, f1_norm, f2_norm)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"audio shape : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMWT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavga\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMWT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Multimodal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1b6408992519>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Feat)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mblock_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mblock_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch_size, window_length, feature_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mblock_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Multimodal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1b6408992519>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, F_block)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# F_block: [batch_size, temporal_window, feature_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# [batch_size, temporal_window, feature_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Multimodal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Multimodal/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Multimodal/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(0, total_epoch):\n",
    "\tepoch_tic = time.time()\n",
    "\tlogging.info(\"Epoch\")\n",
    "\tlogging.info(epoch)\n",
    "\n",
    "\t# train for one epoch\n",
    "\tTraining_vacc, Training_aacc, Training_loss = train(trainloader, model, criterion, optimizer, scheduler, epoch, lr, fusion_model, time_chk_path=time_chk_path)\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
