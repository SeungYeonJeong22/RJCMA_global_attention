{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embed_size, dim, num_layers, dropout, residual_embeddings=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.rnn_dim = dim // 2\n",
    "        self.linear = nn.Linear(dim + embed_size, dim)\n",
    "        self.rnn = nn.LSTM(embed_size, self.rnn_dim, num_layers=num_layers, dropout=dropout,\n",
    "                           bidirectional=True, batch_first=True)\n",
    "        self.residual_embeddings = residual_embeddings\n",
    "        self.init_hidden = nn.Parameter(nn.init.xavier_uniform_(torch.empty(2 * 2 * num_layers, self.rnn_dim)))\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch = inputs.size(0)\n",
    "        h0 = self.init_hidden[:2 * self.num_layers].unsqueeze(1).expand(2 * self.num_layers,\n",
    "                                                                        batch, self.rnn_dim).contiguous()\n",
    "        c0 = self.init_hidden[2 * self.num_layers:].unsqueeze(1).expand(2 * self.num_layers,\n",
    "                                                                        batch, self.rnn_dim).contiguous()\n",
    "\n",
    "        print(\"LSTM inputs : \", inputs.shape)\n",
    "        outputs, hidden_t = self.rnn(inputs, (h0, c0))\n",
    "\n",
    "        if self.residual_embeddings:\n",
    "            outputs = torch.cat([inputs, outputs], dim=-1)\n",
    "        outputs = self.linear(self.dropout(outputs))\n",
    "\n",
    "        return F.normalize(outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "class DenseCoAttn(nn.Module):\n",
    "\tdef __init__(self, dim1, dim2, dropout): #dim1, dim2 = 512, 512\n",
    "\t\tsuper(DenseCoAttn, self).__init__()\n",
    "\t\tdim = dim1 + dim2\n",
    "\t\tself.dropouts = nn.ModuleList([nn.Dropout(p=dropout) for _ in range(2)])\n",
    "\t\tself.query_linear = nn.Linear(dim, dim)\n",
    "\t\tself.key1_linear = nn.Linear(16, 16)\n",
    "\t\tself.key2_linear = nn.Linear(16, 16)\n",
    "\t\tself.value1_linear = nn.Linear(dim1, dim1)\n",
    "\t\tself.value2_linear = nn.Linear(dim2, dim2)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, value1, value2):\n",
    "\t\tprint(\"DenseCoAttn input value1(video) : \", value1.shape) # 16, 16, 512\n",
    "\t\tprint(\"DenseCoAttn input value2(audio) : \", value2.shape)\n",
    "\t\tjoint = torch.cat((value1, value2), dim=-1)\n",
    "\t\t# audio  audio*W*joint\n",
    "\t\tjoint = self.query_linear(joint)\n",
    "\t\tprint(\"DenseCoAttn joint representation : \", joint.shape)\n",
    "\t\tkey1 = self.key1_linear(value1.transpose(1, 2)) # X_v^T\n",
    "\t\tkey2 = self.key2_linear(value2.transpose(1, 2)) # X_a^T \n",
    "\t\tprint(\"DenseCoAttn X_v^T : \", key1.shape) # 16, 512, 16\n",
    "\t\tprint(\"DenseCoAttn X_a^T : \", key2.shape)\n",
    "\n",
    "\t\tvalue1 = self.value1_linear(value1) # 16, 16, 512 (Can't understanding Layer)\n",
    "\t\tvalue2 = self.value2_linear(value2) # (Can't understanding Layer)\n",
    "\t\tprint(\"DenseCoAttn value1 after value_linear : \", value1.shape)\n",
    "\t\tprint(\"DenseCoAttn value2 after value_linear : \", value2.shape)\n",
    "\n",
    "\t\tweighted1, attn1 = self.qkv_attention(joint, key1, value1, dropout=self.dropouts[0])\n",
    "\t\tweighted2, attn2 = self.qkv_attention(joint, key2, value2, dropout=self.dropouts[1])\n",
    "\t\tprint(\"DenseCoAttn weighted1 : \", weighted1.shape)\n",
    "\t\tprint(\"DenseCoAttn weighted2 : \", weighted2.shape)\n",
    "\n",
    "\t\treturn weighted1, weighted2\n",
    "\n",
    "\tdef qkv_attention(self, query, key, value, dropout=None):\n",
    "\t\td_k = query.size(-1)\n",
    "\t\tscores = torch.bmm(key, query) / math.sqrt(d_k)\n",
    "\t\tscores = torch.tanh(scores) # C_v, C_a\n",
    "\t\tif dropout:\n",
    "\t\t\tscores = dropout(scores)\n",
    "\n",
    "\t\tweighted = torch.tanh(torch.bmm(value, scores))\n",
    "\t\treturn self.relu(weighted), scores # self.relu(weighted) == H_v, H_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NormalSubLayer(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout): # dim1, dim2 = 512, 512\n",
    "        super(NormalSubLayer, self).__init__()\n",
    "        self.dense_coattn = DenseCoAttn(dim1, dim2, dropout)\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim1 + dim2, dim1), # 1024, 512\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropout),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim1 + dim2, dim2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropout),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        weighted1, weighted2 = self.dense_coattn(data1, data2) # weighted1, weighted2 = 1024, 1024\n",
    "        data1 = data1 + self.linears[0](weighted1) # X_att,v^t\n",
    "        data2 = data2 + self.linears[1](weighted2) # X_att,a^t\n",
    "\n",
    "        print(\"DCNLayer X_att,v : \" , data1.shape)\n",
    "        print(\"DCNLayer X_att,a : \" , data2.shape)\n",
    "\n",
    "        return data1, data2\n",
    "\n",
    "\n",
    "class DCNLayer(nn.Module):\n",
    "    def __init__(self, dim1, dim2, num_seq, dropout): # dim1, dim2 = 512, 512\n",
    "        super(DCNLayer, self).__init__()\n",
    "        self.dcn_layers = nn.ModuleList([NormalSubLayer(dim1, dim2, dropout) for _ in range(num_seq)]) # 여기서 t-th iteration만큼 계산\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        for dense_coattn in self.dcn_layers:\n",
    "            data1, data2 = dense_coattn(data1, data2)\n",
    "\n",
    "        return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "class BottomUpExtract(nn.Module):\n",
    "\tdef __init__(self, emed_dim, dim):\n",
    "\t\tsuper(BottomUpExtract, self).__init__()\n",
    "\t\tself.attn = PositionAttn(emed_dim, dim)\n",
    "\n",
    "\tdef forward(self, video, audio):\n",
    "\t\tfeat = self.attn(video, audio)\n",
    "\n",
    "\t\treturn feat\n",
    "\n",
    "# audio-guided attention\n",
    "class PositionAttn(nn.Module):\n",
    "\n",
    "\tdef __init__(self, embed_dim, dim):\n",
    "\t\tsuper(PositionAttn, self).__init__()\n",
    "\t\tself.affine_audio = nn.Linear(embed_dim, dim)\n",
    "\t\tself.affine_video = nn.Linear(512, dim)\n",
    "\t\tself.affine_v = nn.Linear(dim, 49, bias=False)\n",
    "\t\tself.affine_g = nn.Linear(dim, 49, bias=False)\n",
    "\t\tself.affine_h = nn.Linear(49, 1, bias=False)\n",
    "\t\tself.affine_feat = nn.Linear(512, dim)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, video, audio):\n",
    "\t\tv_t = video.view(video.size(0) * video.size(1), -1, 512).contiguous()\n",
    "\t\tV = v_t\n",
    "\n",
    "\t\t# Audio-guided visual attention\n",
    "\t\tv_t = self.relu(self.affine_video(v_t))\n",
    "\t\ta_t = audio.view(-1, audio.size(-1))\n",
    "\n",
    "\t\ta_t = self.relu(self.affine_audio(a_t))\n",
    "\n",
    "\t\tcontent_v = self.affine_v(v_t) \\\n",
    "\t\t\t\t\t+ self.affine_g(a_t).unsqueeze(2)\n",
    "\n",
    "\t\tz_t = self.affine_h((torch.tanh(content_v))).squeeze(2)\n",
    "\n",
    "\t\talpha_t = F.softmax(z_t, dim=-1).view(z_t.size(0), -1, z_t.size(1))  # attention map\n",
    "\n",
    "\t\tc_t = torch.bmm(alpha_t, V).view(-1, 512)\n",
    "\t\tvideo_t = c_t.view(video.size(0), -1, 512)\n",
    "\n",
    "\t\tvideo_t = self.affine_feat(video_t)\n",
    "\n",
    "\t\treturn video_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    " \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    " \n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    " \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    " \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "\n",
    "\n",
    "class Global_Attention_Transformer(nn.Module):\n",
    "    def __init__(self, dim=256, dropout=0.6, seq_length=16, n_head=8, n_layer=1) -> None:\n",
    "        super(Global_Attention_Transformer, self).__init__()\n",
    "        self.first_forward = True\n",
    "\n",
    "        self.layer = nn.Linear(dim*2, dim)\n",
    "        \n",
    "        self.pos_enc = PositionalEncoding(dim, dropout, seq_length)\n",
    "\n",
    "        enc_layer1 = nn.TransformerEncoderLayer(d_model=dim, nhead=n_head)\n",
    "        self.encoder1 = nn.TransformerEncoder(enc_layer1, num_layers=n_layer)\n",
    "\n",
    "        enc_layer2 = nn.TransformerEncoderLayer(d_model=dim, nhead=n_head)\n",
    "        self.encoder2 = nn.TransformerEncoder(enc_layer2, num_layers=n_layer)        \n",
    "\n",
    "    def forward(self, data, memory=None):\n",
    "        if not memory==None:\n",
    "            data4 = torch.cat((memory, data), dim=-1)\n",
    "            data3 = self.layer(data4)\n",
    "            data2 = self.pos_enc(data3)\n",
    "            data1 = self.encoder2(data2)\n",
    "        else:\n",
    "            data2 = self.pos_enc(data)\n",
    "            data1 = self.encoder1(data2)\n",
    "\n",
    "        return data1\n",
    "\n",
    "\n",
    "class GAT_LSTM_CAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT_LSTM_CAM, self).__init__()\n",
    "        # self.coattn = DCNLayer(512, 512, 2, 0.6)\n",
    "        self.coattn = DCNLayer(512, 512, 2, 0.6) # 각 모달리티 당 기존 512 + GAT 512 (vid, aud)\n",
    "\n",
    "        self.video_attn = BottomUpExtract(256, 256)\n",
    "        self.audio_extract = LSTM(512, 256, 2, 0.1, residual_embeddings=True)\n",
    "        self.video_extract = LSTM(256, 256, 2, 0.1, residual_embeddings=True)\n",
    "\n",
    "        self.video_GAT = Global_Attention_Transformer()\n",
    "        self.audio_GAT = Global_Attention_Transformer()\n",
    "\n",
    "        self.vregressor = nn.Sequential(nn.Linear(512, 128),\n",
    "                                        nn.ReLU(),\n",
    "                                     nn.Dropout(0.6),\n",
    "                                 nn.Linear(128, 1))\n",
    "\n",
    "\n",
    "        self.aregressor = nn.Sequential(nn.Linear(512, 128),\n",
    "                                        nn.ReLU(),\n",
    "                                     nn.Dropout(0.6),\n",
    "                                 nn.Linear(128, 1))\n",
    "        \n",
    "        # self.Joint = LSTM(1024, 512, 2, dropout=0, residual_embeddings=True)\n",
    "        self.Joint = LSTM(1024, 512, 2, dropout=0, residual_embeddings=True)        \n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(net, init_type='xavier', init_gain=1):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "\n",
    "        def init_func(m):  # define the initialization function\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    init.uniform_(m.weight.data, 0.0, init_gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    init.xavier_uniform_(m.weight.data, gain=init_gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    init.kaiming_uniform_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "                else:\n",
    "                    raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        net.apply(init_func)  # apply the initialization function <init_func>\n",
    "\n",
    "\n",
    "    def forward(self, f1_norm, f2_norm, global_vid=None, global_aud=None):\n",
    "        video = F.normalize(f2_norm, dim=-1)\n",
    "        audio = F.normalize(f1_norm, dim=-1)\n",
    "\n",
    "        # Tried with LSTMs also\n",
    "        audio = self.audio_extract(audio)\n",
    "        video = self.video_attn(video, audio)\n",
    "        video = self.video_extract(video)\n",
    "\n",
    "        global_vid = self.video_GAT(video, global_vid)\n",
    "        global_aud = self.audio_GAT(audio, global_aud)\n",
    "\n",
    "        print(\"video : \", video.shape)\n",
    "        print(\"global_vid : \", global_vid.shape)\n",
    "        print(\"audio : \", audio.shape)\n",
    "        print(\"global_aud : \", global_aud.shape)\n",
    "\n",
    "        gloabl_video = torch.cat((video, global_vid), dim=-1)\n",
    "        gloabl_audio = torch.cat((audio, global_aud), dim=-1)\n",
    "        \n",
    "        gloabl_video, gloabl_audio = self.coattn(gloabl_video, gloabl_audio)\n",
    "\n",
    "        audiovisualfeatures = torch.cat((gloabl_video, gloabl_audio), -1)\n",
    "        \n",
    "        audiovisualfeatures = self.Joint(audiovisualfeatures)\n",
    "        vouts = self.vregressor(audiovisualfeatures) #.transpose(0,1))\n",
    "        aouts = self.aregressor(audiovisualfeatures) #.transpose(0,1))\n",
    "\n",
    "        return vouts.squeeze(2), aouts.squeeze(2), global_vid, global_aud  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tsav import TwoStreamAuralVisualModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'ABAW2020TNT/model2/TSAV_Sub4_544k.pth.tar' # path to the model\n",
    "model = TwoStreamAuralVisualModel(num_channels=4)\n",
    "saved_model = torch.load(model_path)\n",
    "model.load_state_dict(saved_model['state_dict'])\n",
    "\n",
    "new_first_layer = nn.Conv3d(in_channels=3,\n",
    "\t\t\t\t\tout_channels=model.video_model.r2plus1d.stem[0].out_channels,\n",
    "\t\t\t\t\tkernel_size=model.video_model.r2plus1d.stem[0].kernel_size,\n",
    "\t\t\t\t\tstride=model.video_model.r2plus1d.stem[0].stride,\n",
    "\t\t\t\t\tpadding=model.video_model.r2plus1d.stem[0].padding,\n",
    "\t\t\t\t\tbias=False)\n",
    "\n",
    "new_first_layer.weight.data = model.video_model.r2plus1d.stem[0].weight.data[:, 0:3]\n",
    "model.video_model.r2plus1d.stem[0] = new_first_layer\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tmp model shape check\n",
    "# audiodata = torch.empty((16, 16, 1, 64, 104)).cuda()\n",
    "# visualdata = torch.empty((16, 16, 3, 8, 112, 112)).cuda()\n",
    "\n",
    "# with torch.cuda.amp.autocast():\n",
    "#     with torch.no_grad():\n",
    "#         visual_feats = torch.empty((16, 16, 25088), device = visualdata.device)\n",
    "#         aud_feats = torch.empty((16, 16, 512), device = visualdata.device)\n",
    "\n",
    "#         for i in range(16):\n",
    "#             aud_feat, visualfeat, _ = model(audiodata[i,:,:,:], visualdata[i, :, :, :,:,:])\n",
    "#             visual_feats[i,:,:] = visualfeat.view(16, -1)\n",
    "#             aud_feats[i,:,:] = aud_feat\n",
    "\n",
    "# cam = GAT_LSTM_CAM()\n",
    "# result = cam(aud_feats, visual_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils.utils as utils\n",
    "from EvaluationMetrics.cccmetric import ccc\n",
    "\n",
    "import logging\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "learning_rate_decay_start = 5  # 50\n",
    "learning_rate_decay_every = 2 # 5\n",
    "learning_rate_decay_rate = 0.8 # 0.9\n",
    "total_epoch = 30\n",
    "lr = 0.0001\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, scheduler, epoch, lr, cam, time_chk_path):\n",
    "\tprint('\\nEpoch: %d' % epoch)\n",
    "\tglobal Train_acc\n",
    "\t#wandb.watch(audiovisual_model, log_freq=100)\n",
    "\t#wandb.watch(cam, log_freq=100)\n",
    "\n",
    "\t# switch to train mode\n",
    "\t#audiovisual_model.train()\n",
    "\tmodel.eval()\n",
    "\tcam.train()\n",
    "\n",
    "\tepoch_loss = 0\n",
    "\tvout = list()\n",
    "\tvtar = list()\n",
    "\n",
    "\taout = list()\n",
    "\tatar = list()\n",
    "\n",
    "\tif epoch > learning_rate_decay_start and learning_rate_decay_start >= 0:\n",
    "\t\tfrac = (epoch - learning_rate_decay_start) // learning_rate_decay_every\n",
    "\t\tdecay_factor = learning_rate_decay_rate ** frac\n",
    "\t\tcurrent_lr = lr * decay_factor\n",
    "\t\tutils.set_lr(optimizer, current_lr)  # set the decayed rate\n",
    "\telse:\n",
    "\t\tcurrent_lr = lr\n",
    "\t######## chckpoint 없을 때 이거부터 ##########\n",
    "\tutils.set_lr(optimizer, current_lr)\n",
    "\t############################################\n",
    "\tprint('learning_rate: %s' % str(current_lr))\n",
    "\tlogging.info(\"Learning rate\")\n",
    "\tlogging.info(current_lr)\n",
    "\t#torch.cuda.synchronize()\n",
    "\t#t1 = time.time()\n",
    "\tn = 0\n",
    "\tif time_chk_path:\n",
    "\t\ttime_chk_file = os.path.join(time_chk_path, \"time_chk.txt\")\n",
    "  \n",
    "\n",
    "\tfor batch_idx, (visualdata, audiodata, labels_V, labels_A) in tqdm(enumerate(train_loader),\n",
    "\t\t\t\t \t\t\t\t\t\t\t\t\t\t total=len(train_loader), position=0, leave=True):\n",
    "     \n",
    "\t\tprint(\"====\" * 20)\n",
    "\t\tprint(\"Batch Index : \", batch_idx)\n",
    "\t\toptimizer.zero_grad(set_to_none=True)\n",
    "\t\taudiodata = audiodata.cuda()#.unsqueeze(2)\n",
    "\n",
    "\t\tvisualdata = visualdata.cuda()#permute(0,4,1,2,3).cuda()\n",
    "  \n",
    "\t\tst2 = time.time()\n",
    "\n",
    "\n",
    "\t\twith torch.cuda.amp.autocast():\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tb, seq_t, c, subseq_t, h, w = visualdata.size()\n",
    "\t\t\t\tvisual_feats = torch.empty((b, seq_t, 25088), dtype=visualdata.dtype, device = visualdata.device)\n",
    "\t\t\t\taud_feats = torch.empty((b, seq_t, 512), dtype=visualdata.dtype, device = visualdata.device)\n",
    "\n",
    "\t\t\t\tfor i in range(visualdata.shape[0]):\n",
    "\t\t\t\t\tst1 = time.time()\n",
    "\t\t\t\t\taud_feat, visualfeat, _ = model(audiodata[i,:,:,:], visualdata[i, :, :, :,:,:])\n",
    "\t\t\t\t\ted1 = time.time()\n",
    "\n",
    "\t\t\t\t\tpre_trained_model_time = ed1 - st1\n",
    "\t\t\t\t\tif time_chk_path:\n",
    "\t\t\t\t\t\twith open(time_chk_file, 'a') as f:\n",
    "\t\t\t\t\t\t\tf.write(f\"Time pre_trained_model: {pre_trained_model_time}\\n\")\n",
    "\t\t\t\t\t# visual_feats[i,:,:] = visualfeat\n",
    "\t\t\t\t\tvisual_feats[i,:,:] = visualfeat.view(seq_t, -1)\n",
    "\t\t\t\t\taud_feats[i,:,:] = aud_feat\n",
    "\n",
    "\t\t\tst2 = time.time()\n",
    "\t\t\tif batch_idx==0:\n",
    "\t\t\t\taudiovisual_vouts,audiovisual_aouts, global_vid_fts, global_aud_fts = cam(aud_feats, visual_feats)\n",
    "\t\t\telse:\n",
    "\t\t\t\taudiovisual_vouts,audiovisual_aouts, global_vid_fts, global_aud_fts = cam(aud_feats, visual_feats, global_vid_fts, global_aud_fts)\n",
    "\t\t\ted2 = time.time()\n",
    "   \n",
    "\t\t\ttime_cam_model= ed2 - st2\n",
    "\t\t\tif time_chk_path:\n",
    "\t\t\t\twith open(time_chk_file, 'a') as f:\n",
    "\t\t\t\t\tf.write(f\"Time cam model: {time_cam_model}\\n\")\n",
    "\t\t\t\t\tf.write(f\"Epoch: {epoch}\\n\")\n",
    "\t\t\t\t\tf.write(f\"batch_idx: {batch_idx}\\n\")\n",
    "\t\t\t\t\tf.write(\"----\"*20)\n",
    "\t\t\t\t\tf.write(\"\\n\")\n",
    "\t\t\t\tf.close()\n",
    "\n",
    "\t\t\tvoutputs = audiovisual_vouts.view(-1, audiovisual_vouts.shape[0]*audiovisual_vouts.shape[1])\n",
    "\t\t\taoutputs = audiovisual_aouts.view(-1, audiovisual_aouts.shape[0]*audiovisual_aouts.shape[1])\n",
    "\t\t\tvtargets = labels_V.view(-1, labels_V.shape[0]*labels_V.shape[1]).cuda()\n",
    "\t\t\tatargets = labels_A.view(-1, labels_A.shape[0]*labels_A.shape[1]).cuda()\n",
    "   \n",
    "\t\t\tv_loss = criterion(voutputs, vtargets)\n",
    "\t\t\ta_loss = criterion(aoutputs, atargets)\n",
    "   \n",
    "\t\t\tfinal_loss = v_loss + a_loss\n",
    "   \n",
    "\t\t\tepoch_loss += final_loss.cpu().data.numpy()\n",
    "\n",
    "\t\tscaler.scale(final_loss).backward(retain_graph=True)\n",
    "\t\tscaler.step(optimizer)\n",
    "\t\tscaler.update()\n",
    "\t\tn = n + 1\n",
    "\n",
    "\t\tvout = vout + voutputs.squeeze(0).detach().cpu().tolist()\n",
    "\t\tvtar = vtar + vtargets.squeeze(0).detach().cpu().tolist()\n",
    "\n",
    "\t\taout = aout + aoutputs.squeeze(0).detach().cpu().tolist()\n",
    "\t\tatar = atar + atargets.squeeze(0).detach().cpu().tolist()\n",
    "  \n",
    "\tscheduler.step(epoch_loss / n)\n",
    "\n",
    "\tif (len(vtar) > 1):\n",
    "\t\ttrain_vacc = ccc(vout, vtar)\n",
    "\t\ttrain_aacc = ccc(aout, atar)\n",
    "\telse:\n",
    "\t\ttrain_acc = 0\n",
    "\tprint(\"Train Accuracy\")\n",
    "\tprint(train_vacc)\n",
    "\tprint(train_aacc)\n",
    " \n",
    "\treturn train_vacc, train_aacc, final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion Model :  GAT_LSTM_CAM\n",
      "==> Preparing data..\n",
      "Train Data\n",
      "Number of Sequences: 247\n",
      "Val Data\n",
      "Number of Sequences: 82\n",
      "Number of Train samples:68709\n",
      "Number of Val samples:28319\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "from models.tsav import TwoStreamAuralVisualModel\n",
    "from datasets.dataset_new import ImageList\n",
    "from datasets.dataset_val import ImageList_val\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from losses.loss import CCCLoss\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import json\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "is_time_chk = False\n",
    "\n",
    "best_Val_acc = 0  # best PrivateTest accuracy\n",
    "#best_Val_acc = 0  # best PrivateTest accuracy\n",
    "best_Val_acc_epoch = 0\n",
    "\n",
    "TrainingAccuracy_V = []\n",
    "TrainingAccuracy_A = []\n",
    "ValidationAccuracy_V = []\n",
    "ValidationAccuracy_A = []\n",
    "\n",
    "Logfile_name = \"LogFiles/\" + \"log_file.log\"\n",
    "logging.basicConfig(filename=Logfile_name, level=logging.INFO)\n",
    "\n",
    "SEED = int(0)\n",
    "    \n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "class TrainPadSequence:\n",
    "\tdef __call__(self, sorted_batch):\n",
    "\t\tsequences = [x[0] for x in sorted_batch]\n",
    "\t\taud_sequences = [x[1] for x in sorted_batch]\n",
    "\t\tspec_dim = []\n",
    "\n",
    "\t\tfor aud in aud_sequences:\n",
    "\t\t\tspec_dim.append(aud.shape[3])\n",
    "\n",
    "\t\tmax_spec_dim = max(spec_dim)\n",
    "\t\taudio_features = torch.zeros(len(spec_dim), 16, 1, 64, max_spec_dim)\n",
    "\t\tfor batch_idx, spectrogram in enumerate(aud_sequences):\n",
    "\t\t\tif spectrogram.shape[2] < max_spec_dim:\n",
    "\t\t\t\taudio_features[batch_idx, :, :, :, -spectrogram.shape[3]:] = spectrogram\n",
    "\t\t\telse:\n",
    "\t\t\t\taudio_features[batch_idx, :,:, :, :] = spectrogram\n",
    "\n",
    "\t\tlabelV = [x[2] for x in sorted_batch]\n",
    "\t\tlabelA = [x[3] for x in sorted_batch]\n",
    "\t\tvisual_sequences = torch.stack(sequences)\n",
    "\t\tlabelsV = torch.stack(labelV)\n",
    "\t\tlabelsA = torch.stack(labelA)\n",
    "\n",
    "\t\treturn visual_sequences, audio_features, labelsV, labelsA\n",
    "\n",
    "\n",
    "class ValPadSequence:\n",
    "\tdef __call__(self, sorted_batch):\n",
    "\n",
    "\t\tsequences = [x[0] for x in sorted_batch]\n",
    "\t\taud_sequences = [x[1] for x in sorted_batch]\n",
    "\t\tspec_dim = []\n",
    "\t\tfor aud in aud_sequences:\n",
    "\t\t\tspec_dim.append(aud.shape[3])\n",
    "\n",
    "\t\tmax_spec_dim = max(spec_dim)\n",
    "\t\taudio_features = torch.zeros(len(spec_dim), 16, 1, 64, max_spec_dim)\n",
    "\t\tfor batch_idx, spectrogram in enumerate(aud_sequences):\n",
    "\t\t\tif spectrogram.shape[2] < max_spec_dim:\n",
    "\t\t\t\taudio_features[batch_idx, :, :, :, -spectrogram.shape[3]:] = spectrogram\n",
    "\t\t\telse:\n",
    "\t\t\t\taudio_features[batch_idx, :,:, :, :] = spectrogram\n",
    "\n",
    "\t\tframeids = [x[2] for x in sorted_batch]\n",
    "\t\tv_ids = [x[3] for x in sorted_batch]\n",
    "\t\tv_lengths = [x[4] for x in sorted_batch]\n",
    "\t\tlabelV = [x[5] for x in sorted_batch]\n",
    "\t\tlabelA = [x[6] for x in sorted_batch]\n",
    "\n",
    "\t\tvisual_sequences = torch.stack(sequences)\n",
    "\t\tlabelsV = torch.stack(labelV)\n",
    "\t\tlabelsA = torch.stack(labelA)\n",
    "\t\treturn visual_sequences, audio_features, frameids, v_ids, v_lengths, labelsV, labelsA\n",
    "\n",
    "\n",
    "if not os.path.isdir(\"SavedWeights\"):\n",
    "\tos.makedirs(\"SavedWeights\", exist_ok=True)\n",
    "\n",
    "weight_save_path = \"SavedWeights\"\n",
    "\n",
    "result_save_path =\"save\"\n",
    "if not os.path.exists(result_save_path):\n",
    "    os.makedirs(result_save_path)\n",
    "\n",
    "### Loading audiovisual model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = 'ABAW2020TNT/model2/TSAV_Sub4_544k.pth.tar' # path to the model\n",
    "model = TwoStreamAuralVisualModel(num_channels=4)\n",
    "saved_model = torch.load(model_path)\n",
    "model.load_state_dict(saved_model['state_dict'])\n",
    "\n",
    "new_first_layer = nn.Conv3d(in_channels=3,\n",
    "\t\t\t\t\tout_channels=model.video_model.r2plus1d.stem[0].out_channels,\n",
    "\t\t\t\t\tkernel_size=model.video_model.r2plus1d.stem[0].kernel_size,\n",
    "\t\t\t\t\tstride=model.video_model.r2plus1d.stem[0].stride,\n",
    "\t\t\t\t\tpadding=model.video_model.r2plus1d.stem[0].padding,\n",
    "\t\t\t\t\tbias=False)\n",
    "\n",
    "new_first_layer.weight.data = model.video_model.r2plus1d.stem[0].weight.data[:, 0:3]\n",
    "model.video_model.r2plus1d.stem[0] = new_first_layer\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "### Freezing the model\n",
    "for p in model.parameters():\n",
    "\tp.requires_grad = False\n",
    "for p in model.children():\n",
    "\tp.train(False)\n",
    " \n",
    "fusion_model = GAT_LSTM_CAM()\n",
    "\n",
    "print_model_name = fusion_model.__class__.__name__\n",
    "print(\"Fusion Model : \", print_model_name)\n",
    "\n",
    "fusion_model = fusion_model.to(device=device)\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "def matching_files(root_path, anno_path):\n",
    "\tanno_list = []\n",
    "\tfor f in os.listdir(anno_path):\n",
    "\t\tanno_list.append(f.split(\".\")[0])\n",
    "  \n",
    "\troot_path_list = os.listdir(root_path)\n",
    "\t\n",
    "\tfor f in os.listdir(root_path):\n",
    "\t\tif not f in anno_list:\n",
    "\t\t\tdel root_path_list[root_path_list.index(f)]\n",
    "\n",
    "\treturn root_path_list\n",
    "\n",
    "def train_val_test_split(root_path, anno_path, seed=0):\n",
    "\trandom.seed(seed)\n",
    "\ttrial_data = matching_files(root_path, anno_path)\n",
    " \n",
    "\tfname_dict = {i:f for i,f in enumerate(trial_data)}\n",
    "\tlength = len(fname_dict)\n",
    " \n",
    "\tprint(\"full trial length: \", len(fname_dict))\n",
    "\n",
    "\ttrain_set = []\n",
    "\tvalid_set = []\n",
    "\ttest_set = []\n",
    " \n",
    "\ttrain_list_idx = random.sample(fname_dict.keys(), int(length*0.6))\n",
    "\tfor i in train_list_idx:\n",
    "\t\ttrain_set.append(fname_dict[i]+\".csv\")\n",
    "\t\tdel fname_dict[i]\n",
    "\t\t\n",
    "\tvalid_list_idx = random.sample(fname_dict.keys(), int(length*0.2))\n",
    "\tfor i in valid_list_idx:\n",
    "\t\tvalid_set.append(fname_dict[i]+\".csv\")\n",
    "\t\tdel fname_dict[i]\n",
    "\n",
    "\ttest_list_idx = random.sample(fname_dict.keys(), int(length*0.2))    \n",
    "\tfor i in test_list_idx:\n",
    "\t\ttest_set.append(fname_dict[i]+\".csv\")\n",
    "\t\tdel fname_dict[i]\n",
    "  \n",
    "\treturn train_set, valid_set, test_set\n",
    "    \n",
    "with open('config_file.json', 'r') as f:\n",
    "\tconfiguration = json.load(f)\n",
    "\n",
    "dataset_rootpath = configuration['dataset_rootpath']\n",
    "dataset_wavspath = configuration['dataset_wavspath']\n",
    "dataset_labelpath = configuration['labelpath']\n",
    "\n",
    "def load_partition_set(partition_path, seed):\n",
    "\timport json\n",
    "\n",
    "\twith open(partition_path, 'r') as f:    \n",
    "\t\tseed_data = json.load(f)\n",
    "\n",
    "\tseed_data_train = seed_data[f'seed_{seed}']['Train_Set']\n",
    "\tseed_data_valid = seed_data[f'seed_{seed}']['Validation_Set']\n",
    "\tseed_data_test  = seed_data[f'seed_{seed}']['Test_Set']\n",
    " \n",
    "\tseed_data_train = [fn + \".csv\" for fn in seed_data_train]\n",
    "\tseed_data_valid = [fn + \".csv\" for fn in seed_data_valid]\n",
    "\tseed_data_test  = [fn + \".csv\" for fn in seed_data_test ]\n",
    "\n",
    "\treturn seed_data_train, seed_data_valid, seed_data_test\n",
    "\n",
    "partition_path = \"../data/Affwild2/seed_data.json\"\n",
    " \n",
    "train_set, valid_set, test_set = load_partition_set(partition_path, SEED)\n",
    "\n",
    "init_time = datetime.now()\n",
    "init_time = init_time.strftime('%m%d_%H%M')\n",
    "\n",
    "root_time_chk_dir = \"time_chk\"\n",
    "\n",
    "time_chk_path = None\n",
    "\n",
    "print(\"Train Data\")\n",
    "traindataset = ImageList(root=configuration['dataset_rootpath'], fileList=train_set, labelPath=dataset_labelpath,\n",
    "                        audList=configuration['dataset_wavspath'], length=configuration['train_params']['seq_length'],\n",
    "                        flag='train', stride=configuration['train_params']['stride'], dilation = configuration['train_params']['dilation'],\n",
    "                        subseq_length = configuration['train_params']['subseq_length'], time_chk_path=time_chk_path)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                traindataset, collate_fn=TrainPadSequence(),\n",
    "                **configuration['train_params']['loader_params'])\n",
    "print(\"Number of Train samples:\" + str(len(traindataset)))\n",
    "\n",
    "criterion = CCCLoss(digitize_num=1).cuda()\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(),# filter(lambda p: p.requires_grad, multimedia_model.parameters()),\n",
    "\t\t\t\t\t\t\t\tconfiguration['model_params']['lr'])\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "cnt = 0\n",
    "fusion_model_name = 'gat'\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "learning_rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4295 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Batch Index :  0\n",
      "LSTM inputs :  torch.Size([16, 16, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4295 [00:09<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM inputs :  torch.Size([16, 16, 256])\n",
      "video :  torch.Size([16, 16, 256])\n",
      "global_vid :  torch.Size([16, 16, 256])\n",
      "audio :  torch.Size([16, 16, 256])\n",
      "global_aud :  torch.Size([16, 16, 256])\n",
      "DenseCoAttn input value1(video) :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn input value2(audio) :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn joint representation :  torch.Size([16, 16, 1024])\n",
      "DenseCoAttn X_v^T :  torch.Size([16, 512, 16])\n",
      "DenseCoAttn X_a^T :  torch.Size([16, 512, 16])\n",
      "DenseCoAttn value1 after value_linear :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn value2 after value_linear :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn weighted1 :  torch.Size([16, 16, 1024])\n",
      "DenseCoAttn weighted2 :  torch.Size([16, 16, 1024])\n",
      "DCNLayer X_att,v :  torch.Size([16, 16, 512])\n",
      "DCNLayer X_att,a :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn input value1(video) :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn input value2(audio) :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn joint representation :  torch.Size([16, 16, 1024])\n",
      "DenseCoAttn X_v^T :  torch.Size([16, 512, 16])\n",
      "DenseCoAttn X_a^T :  torch.Size([16, 512, 16])\n",
      "DenseCoAttn value1 after value_linear :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn value2 after value_linear :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn weighted1 :  torch.Size([16, 16, 1024])\n",
      "DenseCoAttn weighted2 :  torch.Size([16, 16, 1024])\n",
      "DCNLayer X_att,v :  torch.Size([16, 16, 512])\n",
      "DCNLayer X_att,a :  torch.Size([16, 16, 512])\n",
      "LSTM inputs :  torch.Size([16, 16, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'StdBackward' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8bffedfc06ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mTraining_vacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTraining_aacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfusion_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_chk_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_chk_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-f744e565de52>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, scheduler, epoch, lr, cam, time_chk_path)\u001b[0m\n\u001b[1;32m    121\u001b[0m                         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfinal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Multimodal/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Multimodal/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'StdBackward' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "for epoch in range(0, total_epoch):\n",
    "\tepoch_tic = time.time()\n",
    "\tlogging.info(\"Epoch\")\n",
    "\tlogging.info(epoch)\n",
    "\n",
    "\t# train for one epoch\n",
    "\tTraining_vacc, Training_aacc, Training_loss = train(trainloader, model, criterion, optimizer, scheduler, epoch, lr, fusion_model, time_chk_path=time_chk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
